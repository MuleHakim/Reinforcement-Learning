{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONBNqf8YaSf8kHE1cUsH50",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuleHakim/Reinforcement-Learning/blob/main/Policy_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pollicy Iteration"
      ],
      "metadata": {
        "id": "Ngsa2oS9RB2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid World (FrozenLake Environment)"
      ],
      "metadata": {
        "id": "-8rS6XaSRFpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afC9dZnBQ8LU",
        "outputId": "b96cb26b-e343-4852-93fb-dcce1cd8a6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.9/953.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "FW6ANsM7RNXy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)"
      ],
      "metadata": {
        "id": "jnA5wxMFRVHC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, env, gamma=0.99, theta=1e-8):\n",
        "    # Initialize the value table with zeros\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        # Iterate over all states in the environment\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "            # Compute the value of the current policy\n",
        "            value = sum([prob * (reward + gamma * value_table[next_state])\n",
        "                         for prob, next_state, reward, _ in env.P[state][action]])\n",
        "            # Update delta to check convergence\n",
        "            delta = max(delta, np.abs(value - value_table[state]))\n",
        "            # Update the value table\n",
        "            value_table[state] = value\n",
        "\n",
        "        # Check if the values have converged\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return value_table\n"
      ],
      "metadata": {
        "id": "nUxehviiRXup"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    # Initialize a random policy\n",
        "    policy = np.random.choice(env.action_space.n, env.observation_space.n)\n",
        "\n",
        "    while True:\n",
        "        # Evaluate the current policy\n",
        "        value_table = policy_evaluation(policy, env, gamma, theta)\n",
        "        policy_stable = True\n",
        "\n",
        "        # Iterate over all states to improve the policy\n",
        "        for state in range(env.observation_space.n):\n",
        "            old_action = policy[state]\n",
        "            # Calculate Q-value for each action in the current state\n",
        "            Q_values = [sum([prob * (reward + gamma * value_table[next_state])\n",
        "                             for prob, next_state, reward, _ in env.P[state][action]])\n",
        "                        for action in range(env.action_space.n)]\n",
        "            # Choose the action with the highest Q-value\n",
        "            new_action = np.argmax(Q_values)\n",
        "            if old_action != new_action:\n",
        "                policy_stable = False\n",
        "            policy[state] = new_action\n",
        "\n",
        "        # If the policy is stable, break the loop\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return policy, value_table"
      ],
      "metadata": {
        "id": "EuSVq_Z7RaNN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, value_table = policy_iteration(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1esHgMiRbt-",
        "outputId": "cc15a5af-99cb-40a5-f2c6-7e4b32900cee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal Policy:\", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M14d3rESRdMB",
        "outputId": "9cdb3ecc-55c7-4433-9c15-0c8c205aa125"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Value Table:\", value_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdpAuKFWReOm",
        "outputId": "dd52611f-21ec-4c8d-fb1e-c4dc8ffab948"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Table: [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
            " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
            " 0.         0.99       1.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-09KnSJHRfs-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6jiDTuyRgzC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single-State Multi-Armed Bandit\n"
      ],
      "metadata": {
        "id": "jIP0vDyCRokL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration_bandit(k, gamma=0.99, theta=1e-8, max_steps=1000):\n",
        "    policy = np.random.choice(k)\n",
        "    value_table = np.zeros(k)\n",
        "    rewards = np.random.randn(k)  # Assume normal distribution with unit variance\n",
        "\n",
        "    while True:\n",
        "        old_policy = policy\n",
        "\n",
        "        # Policy evaluation\n",
        "        for step in range(max_steps):\n",
        "            new_value_table = np.copy(value_table)\n",
        "            for action in range(k):\n",
        "                new_value_table[action] = rewards[action] + gamma * np.max(value_table)\n",
        "            if np.max(np.abs(new_value_table - value_table)) < theta:\n",
        "                break\n",
        "            value_table = new_value_table\n",
        "\n",
        "        # Policy improvement\n",
        "        policy = np.argmax(value_table)\n",
        "\n",
        "        if policy == old_policy:\n",
        "            break\n",
        "\n",
        "    return policy, value_table"
      ],
      "metadata": {
        "id": "p_2tYF8jRq6Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10  # Number of arms"
      ],
      "metadata": {
        "id": "3dS0JRcVRvad"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, value_table = policy_iteration_bandit(k)"
      ],
      "metadata": {
        "id": "jFsYa3-yRwwq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal Policy for Bandit:\", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBnYnN5Ry27",
        "outputId": "3160d5cc-322d-476b-d3ed-c8681c12dd48"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy for Bandit: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Value Table for Bandit:\", value_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwrooTaNSYaD",
        "outputId": "f4d9de0a-e2e0-472f-8124-bbb0a24fbe6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Table for Bandit: [210.18299157 209.98884411 210.33777636 209.85741078 209.17872026\n",
            " 207.43406794 211.05546753 211.99305777 208.93840636 210.16512246]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CAUGFqkQSY4j"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}