{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5ZSvRfAX/S8AYu3HmxguC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuleHakim/Reinforcement-Learning/blob/main/Value_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Value Iteration"
      ],
      "metadata": {
        "id": "k_rREpOPQbUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Grid World (FrozenLake Environment)"
      ],
      "metadata": {
        "id": "zkwM5jfbQSlH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbUpL_WCPYiD",
        "outputId": "6b677712-45fe-4b17-e2e1-94ee38936523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "wRoisghKP6k5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=False)"
      ],
      "metadata": {
        "id": "hZ0ncUpqP9gT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.observation_space.n):\n",
        "            Q_values = [sum([prob * (reward + gamma * value_table[next_state])\n",
        "                             for prob, next_state, reward, _ in env.P[state][action]])\n",
        "                        for action in range(env.action_space.n)]\n",
        "            max_Q_value = max(Q_values)\n",
        "            delta = max(delta, np.abs(max_Q_value - value_table[state]))\n",
        "            value_table[state] = max_Q_value\n",
        "        if delta < theta:\n",
        "            break\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "    for state in range(env.observation_space.n):\n",
        "        Q_values = [sum([prob * (reward + gamma * value_table[next_state])\n",
        "                         for prob, next_state, reward, _ in env.P[state][action]])\n",
        "                    for action in range(env.action_space.n)]\n",
        "        policy[state] = np.argmax(Q_values)\n",
        "    return policy, value_table"
      ],
      "metadata": {
        "id": "dGu6fTO2P_iE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, value_table = value_iteration(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAE20BlMQCQ1",
        "outputId": "cc71c145-8f03-4c83-c8b7-178ffb5d37fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal Policy:\", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7hPp_ltQEOD",
        "outputId": "930e0a67-cc2d-4400-d1f1-2df46a4862d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Value Table:\", value_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAubyJS-QF1e",
        "outputId": "c56dd8f6-4385-45a4-8c91-f1c02d5d5ce6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Table: [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
            " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
            " 0.         0.99       1.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDDwikXeQIqX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOtq8OWmPpsX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single-State Multi-Armed Bandit"
      ],
      "metadata": {
        "id": "HImKWCp2QgZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration_bandit(k, gamma=0.99, theta=1e-8, max_steps=1000):\n",
        "    # Initialize value table for each arm\n",
        "    value_table = np.zeros(k)\n",
        "    rewards = np.random.randn(k)  # Assume normal distribution with unit variance\n",
        "    for step in range(max_steps):\n",
        "        for action in range(k):\n",
        "            value_table[action] = (1 / (step + 1)) * (rewards[action] + gamma * max(value_table))\n",
        "    return value_table\n"
      ],
      "metadata": {
        "id": "hscEyseGQjXy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10  # Number of arms"
      ],
      "metadata": {
        "id": "b-nvUjzwQn19"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_table = value_iteration_bandit(k)"
      ],
      "metadata": {
        "id": "zQlhnZZJQr51"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Value Table for Bandit:\", value_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmqM1LM9Qs0_",
        "outputId": "6a4d04b1-910c-4cae-acc9-38ad9c62a6b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Table for Bandit: [-0.00038309 -0.00063805  0.00087915 -0.00021576  0.00019603 -0.00052959\n",
            "  0.0009182   0.00032485 -0.00090633 -0.00233692]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQ4zwyZfQt5o"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}